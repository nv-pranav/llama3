{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-05T01:10:11.218284Z",
     "start_time": "2024-06-05T01:09:08.413008Z"
    }
   },
   "source": [
    "import torch\n",
    "from dummyModelL3 import Transformer, ModelArgs\n",
    "default_dtype = torch.get_default_dtype()\n",
    "print(default_dtype)  # This will print the default dtype\n",
    "def main():\n",
    "    # Initialize model arguments\n",
    "    model_args = ModelArgs(\n",
    "        dim=4096,\n",
    "        n_layers=32,\n",
    "        n_heads=32,\n",
    "        vocab_size=30522,  # Set the appropriate vocab size for your model\n",
    "        multiple_of=256,\n",
    "        ffn_dim_multiplier=None,\n",
    "        norm_eps=1e-5,\n",
    "        max_batch_size=32,\n",
    "        max_seq_len=2048,\n",
    "        rope_theta=500000  # Add this parameter for the second model\n",
    "    )\n",
    "\n",
    "    # Initialize the model\n",
    "    model = Transformer(model_args).cuda()  # Move model to GPU if available\n",
    "    \n",
    "    for name, layer in model.named_modules():\n",
    "        print(name, layer)\n",
    "        \n",
    "    # Create dummy input (batch_size=1, sequence_length=128)\n",
    "    dummy_input = torch.randint(0, model_args.vocab_size, (1, 128)).cuda()\n",
    "    \n",
    "    # Run the forward pass and print layer details\n",
    "    output = model(dummy_input, start_pos=0)\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "main()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n",
      " Transformer(\n",
      "  (tok_embeddings): Embedding(30522, 4096)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x TransformerBlock(\n",
      "      (attention): Attention(\n",
      "        (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "        (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "        (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "      )\n",
      "      (attention_norm): RMSNorm()\n",
      "      (ffn_norm): RMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): RMSNorm()\n",
      "  (output): Linear(in_features=4096, out_features=30522, bias=False)\n",
      ")\n",
      "tok_embeddings Embedding(30522, 4096)\n",
      "layers ModuleList(\n",
      "  (0-31): 32 x TransformerBlock(\n",
      "    (attention): Attention(\n",
      "      (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    )\n",
      "    (feed_forward): FeedForward(\n",
      "      (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "      (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "      (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    )\n",
      "    (attention_norm): RMSNorm()\n",
      "    (ffn_norm): RMSNorm()\n",
      "  )\n",
      ")\n",
      "layers.0 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.0.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.0.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.0.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.0.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.0.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.0.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.0.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.0.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.0.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.0.attention_norm RMSNorm()\n",
      "layers.0.ffn_norm RMSNorm()\n",
      "layers.1 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.1.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.1.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.1.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.1.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.1.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.1.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.1.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.1.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.1.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.1.attention_norm RMSNorm()\n",
      "layers.1.ffn_norm RMSNorm()\n",
      "layers.2 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.2.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.2.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.2.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.2.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.2.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.2.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.2.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.2.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.2.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.2.attention_norm RMSNorm()\n",
      "layers.2.ffn_norm RMSNorm()\n",
      "layers.3 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.3.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.3.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.3.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.3.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.3.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.3.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.3.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.3.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.3.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.3.attention_norm RMSNorm()\n",
      "layers.3.ffn_norm RMSNorm()\n",
      "layers.4 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.4.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.4.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.4.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.4.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.4.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.4.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.4.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.4.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.4.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.4.attention_norm RMSNorm()\n",
      "layers.4.ffn_norm RMSNorm()\n",
      "layers.5 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.5.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.5.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.5.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.5.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.5.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.5.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.5.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.5.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.5.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.5.attention_norm RMSNorm()\n",
      "layers.5.ffn_norm RMSNorm()\n",
      "layers.6 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.6.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.6.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.6.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.6.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.6.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.6.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.6.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.6.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.6.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.6.attention_norm RMSNorm()\n",
      "layers.6.ffn_norm RMSNorm()\n",
      "layers.7 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.7.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.7.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.7.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.7.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.7.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.7.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.7.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.7.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.7.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.7.attention_norm RMSNorm()\n",
      "layers.7.ffn_norm RMSNorm()\n",
      "layers.8 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.8.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.8.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.8.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.8.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.8.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.8.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.8.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.8.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.8.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.8.attention_norm RMSNorm()\n",
      "layers.8.ffn_norm RMSNorm()\n",
      "layers.9 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.9.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.9.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.9.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.9.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.9.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.9.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.9.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.9.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.9.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.9.attention_norm RMSNorm()\n",
      "layers.9.ffn_norm RMSNorm()\n",
      "layers.10 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.10.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.10.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.10.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.10.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.10.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.10.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.10.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.10.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.10.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.10.attention_norm RMSNorm()\n",
      "layers.10.ffn_norm RMSNorm()\n",
      "layers.11 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.11.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.11.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.11.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.11.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.11.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.11.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.11.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.11.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.11.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.11.attention_norm RMSNorm()\n",
      "layers.11.ffn_norm RMSNorm()\n",
      "layers.12 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.12.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.12.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.12.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.12.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.12.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.12.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.12.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.12.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.12.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.12.attention_norm RMSNorm()\n",
      "layers.12.ffn_norm RMSNorm()\n",
      "layers.13 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.13.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.13.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.13.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.13.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.13.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.13.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.13.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.13.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.13.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.13.attention_norm RMSNorm()\n",
      "layers.13.ffn_norm RMSNorm()\n",
      "layers.14 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.14.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.14.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.14.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.14.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.14.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.14.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.14.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.14.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.14.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.14.attention_norm RMSNorm()\n",
      "layers.14.ffn_norm RMSNorm()\n",
      "layers.15 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.15.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.15.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.15.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.15.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.15.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.15.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.15.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.15.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.15.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.15.attention_norm RMSNorm()\n",
      "layers.15.ffn_norm RMSNorm()\n",
      "layers.16 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.16.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.16.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.16.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.16.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.16.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.16.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.16.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.16.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.16.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.16.attention_norm RMSNorm()\n",
      "layers.16.ffn_norm RMSNorm()\n",
      "layers.17 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.17.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.17.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.17.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.17.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.17.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.17.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.17.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.17.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.17.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.17.attention_norm RMSNorm()\n",
      "layers.17.ffn_norm RMSNorm()\n",
      "layers.18 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.18.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.18.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.18.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.18.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.18.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.18.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.18.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.18.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.18.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.18.attention_norm RMSNorm()\n",
      "layers.18.ffn_norm RMSNorm()\n",
      "layers.19 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.19.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.19.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.19.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.19.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.19.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.19.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.19.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.19.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.19.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.19.attention_norm RMSNorm()\n",
      "layers.19.ffn_norm RMSNorm()\n",
      "layers.20 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.20.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.20.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.20.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.20.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.20.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.20.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.20.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.20.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.20.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.20.attention_norm RMSNorm()\n",
      "layers.20.ffn_norm RMSNorm()\n",
      "layers.21 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.21.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.21.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.21.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.21.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.21.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.21.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.21.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.21.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.21.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.21.attention_norm RMSNorm()\n",
      "layers.21.ffn_norm RMSNorm()\n",
      "layers.22 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.22.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.22.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.22.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.22.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.22.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.22.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.22.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.22.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.22.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.22.attention_norm RMSNorm()\n",
      "layers.22.ffn_norm RMSNorm()\n",
      "layers.23 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.23.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.23.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.23.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.23.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.23.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.23.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.23.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.23.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.23.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.23.attention_norm RMSNorm()\n",
      "layers.23.ffn_norm RMSNorm()\n",
      "layers.24 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.24.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.24.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.24.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.24.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.24.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.24.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.24.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.24.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.24.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.24.attention_norm RMSNorm()\n",
      "layers.24.ffn_norm RMSNorm()\n",
      "layers.25 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.25.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.25.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.25.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.25.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.25.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.25.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.25.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.25.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.25.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.25.attention_norm RMSNorm()\n",
      "layers.25.ffn_norm RMSNorm()\n",
      "layers.26 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.26.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.26.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.26.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.26.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.26.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.26.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.26.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.26.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.26.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.26.attention_norm RMSNorm()\n",
      "layers.26.ffn_norm RMSNorm()\n",
      "layers.27 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.27.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.27.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.27.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.27.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.27.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.27.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.27.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.27.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.27.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.27.attention_norm RMSNorm()\n",
      "layers.27.ffn_norm RMSNorm()\n",
      "layers.28 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.28.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.28.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.28.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.28.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.28.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.28.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.28.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.28.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.28.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.28.attention_norm RMSNorm()\n",
      "layers.28.ffn_norm RMSNorm()\n",
      "layers.29 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.29.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.29.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.29.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.29.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.29.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.29.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.29.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.29.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.29.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.29.attention_norm RMSNorm()\n",
      "layers.29.ffn_norm RMSNorm()\n",
      "layers.30 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.30.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.30.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.30.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.30.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.30.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.30.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.30.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.30.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.30.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.30.attention_norm RMSNorm()\n",
      "layers.30.ffn_norm RMSNorm()\n",
      "layers.31 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.31.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.31.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.31.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.31.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.31.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.31.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.31.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.31.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.31.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.31.attention_norm RMSNorm()\n",
      "layers.31.ffn_norm RMSNorm()\n",
      "norm RMSNorm()\n",
      "output Linear(in_features=4096, out_features=30522, bias=False)\n",
      "Output shape: torch.Size([1, 128, 30522])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T01:08:26.407103Z",
     "start_time": "2024-06-05T01:08:26.404363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the default dtype globally\n",
    "default_dtype = torch.get_default_dtype()\n",
    "print(default_dtype)  # This will print the default dtype\n",
    "\n"
   ],
   "id": "eccd40401415a597",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4347cbaf65feb7c3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
